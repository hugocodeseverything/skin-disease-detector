# -*- coding: utf-8 -*-
"""Kelompok8_Project_Revised_XGBoost_Extreme.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C9yKbfk7r30TZ5Gh3es_s8K4O6TFCVNm

# **Step 0.1:Import Library**
"""

import os
import glob
from io import BytesIO
from collections import Counter

import numpy as np
import pandas as pd
import requests
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from PIL import Image

from skimage.io import imread
from skimage.transform import resize
from skimage.color import rgb2gray, rgb2hsv, rgb2lab
from skimage import exposure, measure
from skimage.filters import (
    gaussian,
    unsharp_mask,
    threshold_otsu,
)
from skimage.morphology import opening, disk
from skimage.feature import (
    hog,
    local_binary_pattern,
    graycomatrix,
    graycoprops,
)

from sklearn.preprocessing import (
    StandardScaler,
    Normalizer,
    LabelEncoder,
    label_binarize,
)
from sklearn.decomposition import PCA
from sklearn.model_selection import (
    train_test_split,
    RandomizedSearchCV,
)
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
    roc_curve,
    auc,
    precision_recall_curve,
)
from sklearn.manifold import TSNE

from xgboost import XGBClassifier

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%javascript
# function keepAlive() {
#   console.log("Ping...");
#   window.dispatchEvent(new KeyboardEvent('keydown', {'key':'Shift'}));
# }
# setInterval(keepAlive, 60000);

"""# **Step 1.1: Download The Dataset and Check the Information**"""

import kagglehub
path = kagglehub.dataset_download("shubhamgoel27/dermnet")
print("Path to dataset files:", path)

import os
DATASET_PATH = path
print(os.listdir(DATASET_PATH))

import os

DATASET_PATH = DATASET_PATH + "/train"

USE_CLASSES = sorted([
    d for d in os.listdir(DATASET_PATH)
    if os.path.isdir(os.path.join(DATASET_PATH, d))
])

print("Total classes:", len(USE_CLASSES))
print(USE_CLASSES)

import os

class_counts = {}
for cls in USE_CLASSES:
    folder_path = os.path.join(DATASET_PATH, cls)
    if os.path.isdir(folder_path):
        n = len(os.listdir(folder_path))
        class_counts[cls] = n
        print(f"{cls:60s}: {n}")
    else:
        print(f"{cls} Not Found")
print("\nSorted (smallest → biggest):")
print(dict(sorted(class_counts.items(), key=lambda x: x[1])))

import os

all_classes = sorted(os.listdir(DATASET_PATH))

class_counts = {}

for cls in all_classes:
    folder_path = os.path.join(DATASET_PATH, cls)
    if os.path.isdir(folder_path):
        n = len(os.listdir(folder_path))
        class_counts[cls] = n
        print(f"{cls:60s}: {n}")

print("\nSorted (smallest → biggest):")
for k, v in sorted(class_counts.items(), key=lambda x: x[1]):
    print(f"{k:60s}: {v}")

"""# **Step 2: Preprocess Image**"""

# ============================================
# 1. IMPROVED PREPROCESSING
# ============================================
from skimage.color import rgb2gray, rgb2hsv, hsv2rgb
from skimage.exposure import equalize_adapthist, rescale_intensity
from skimage.transform import resize
from skimage.io import imread
from skimage.filters import gaussian
import numpy as np

def advanced_preprocess(path_or_img):
    """Preprocess gambar dengan normalisasi lebih baik."""
    # Load image
    if isinstance(path_or_img, str):
        img = imread(path_or_img)
    else:
        img = path_or_img

    # Resize
    img = resize(img, (256, 256), anti_aliasing=True)

    # CLAHE on V channel (lebih kuat)
    hsv = rgb2hsv(img)
    hsv[..., 2] = equalize_adapthist(hsv[..., 2], clip_limit=0.03)  # 0.02 -> 0.03
    img = hsv2rgb(hsv)

    # Contrast stretching (tambahan - penting!)
    for i in range(3):
        p2, p98 = np.percentile(img[..., i], (2, 98))
        img[..., i] = rescale_intensity(img[..., i], in_range=(p2, p98))

    # Normalize to [0, 1]
    img = np.clip(img, 0, 1)

    # Denoise sedikit
    img = gaussian(img, sigma=0.5, channel_axis=-1)

    return img

"""# **Step 3.1:Augmentation Image**"""

import random
from skimage.transform import rotate
from skimage.exposure import adjust_gamma
from skimage.util import random_noise

def augment_image(img):
    aug_list = []
    augmentation_pool = []

    # Rotate
    for angle in [10, -10, 15, -15]:
        augmentation_pool.append(('rotate', angle))

    # Flip
    augmentation_pool.append(('flip_h', None))
    augmentation_pool.append(('flip_v', None))

    # Gamma
    for gamma in [0.8, 0.9, 1.1, 1.2]:
        augmentation_pool.append(('gamma', gamma))

    # Brightness
    for brightness in [0.9, 1.1]:
        augmentation_pool.append(('brightness', brightness))

    # Noise
    for variance in [0.002, 0.004]:
        augmentation_pool.append(('noise', variance))

    # Pilih random 3-5 augmentasi
    num_augs = random.randint(3, 5)
    selected = random.sample(augmentation_pool, num_augs)

    # Apply augmentasi
    for aug_type, param in selected:
        if aug_type == 'rotate':
            aug_list.append(np.clip(rotate(img, param, mode='reflect', preserve_range=True), 0, 1))
        elif aug_type == 'flip_h':
            aug_list.append(np.fliplr(img))
        elif aug_type == 'flip_v':
            aug_list.append(np.flipud(img))
        elif aug_type == 'gamma':
            aug_list.append(np.clip(adjust_gamma(img, gamma=param), 0, 1))
        elif aug_type == 'brightness':
            aug_list.append(np.clip(img * param, 0, 1))
        elif aug_type == 'noise':
            aug_list.append(np.clip(random_noise(img, mode='gaussian', var=param), 0, 1))

    return aug_list

"""# **Step 3.2: Feature Extraction**"""

from skimage.feature import local_binary_pattern, hog, graycomatrix, graycoprops
from skimage.filters import threshold_otsu
from skimage import measure
from scipy.stats import skew

def extract_features_from_image(img):
    gray = rgb2gray(img)
    gray_uint8 = (gray * 255).astype(np.uint8)

    # LBP
    lbp = local_binary_pattern(gray_uint8, P=16, R=2, method="uniform")
    lbp_hist, _ = np.histogram(lbp.ravel(), bins=18, range=(0, 18), density=True)

    # GLCM
    gray_q = (gray_uint8 // 8).astype(np.uint8)
    glcm = graycomatrix(
        gray_q,
        distances=[1, 2],
        angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],
        levels=32,
        symmetric=True,
        normed=True
    )
    props = ["contrast", "dissimilarity", "homogeneity", "energy", "correlation", "ASM"]
    glcm_features = np.hstack([graycoprops(glcm, p).ravel() for p in props])

    # HOG
    small_gray = resize(gray, (64, 64), anti_aliasing=True)
    hog_feat = hog(
        small_gray,
        orientations=9,
        pixels_per_cell=(16, 16),
        cells_per_block=(2, 2),
        block_norm="L2-Hys",
        feature_vector=True
    )

    # HSV histogram
    hsv = rgb2hsv(img)
    hsv_hist = np.concatenate([
        np.histogram(hsv[..., 0], bins=32, range=(0, 1), density=True)[0],
        np.histogram(hsv[..., 1], bins=16, range=(0, 1), density=True)[0],
        np.histogram(hsv[..., 2], bins=16, range=(0, 1), density=True)[0],
    ])

    # Color moments
    color_moments = []
    for channel in range(3):
        channel_data = img[..., channel].ravel()
        color_moments.extend([
            np.mean(channel_data),
            np.std(channel_data),
            skew(channel_data, bias=False)
        ])
    color_moments = np.array(color_moments)

    # Shape features
    try:
        thresh = threshold_otsu(gray)
        mask = (gray > thresh).astype(np.uint8)
        labeled = measure.label(mask)
        props = measure.regionprops(labeled)

        if props:
            largest = max(props, key=lambda x: x.area)
            area = largest.area / (img.shape[0] * img.shape[1])
            perimeter = largest.perimeter
            eccentricity = largest.eccentricity
            solidity = largest.solidity

            if perimeter > 0:
                compactness = (4 * np.pi * largest.area) / (perimeter ** 2)
            else:
                compactness = 0.0
        else:
            area = perimeter = eccentricity = solidity = compactness = 0.0
    except:
        area = perimeter = eccentricity = solidity = compactness = 0.0

    features = np.concatenate([
        lbp_hist,
        glcm_features,
        hog_feat,
        hsv_hist,
        color_moments,
        np.array([area, eccentricity, solidity, compactness, perimeter])
    ])

    return features

"""# **Step 4:Convert All Images to X and Y, Scaling, Train Test Split, PCA**"""

import os
import glob
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from skimage.io import imread
from skimage.transform import resize
from skimage.color import rgb2hsv, hsv2rgb
from skimage.exposure import equalize_adapthist

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

import xgboost as xgb
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score,
    classification_report, confusion_matrix
)


# 2. EXTRACT FEATURES DARI PATH (menggunakan fungsi manual)
def extract_features_from_path(path):
    img = advanced_preprocess(path)
    return extract_features_from_image(img)

# 3. LOAD DATA & EXTRACT FEATURES

X = []
y_list = []
paths = []

print("Extracting features (augment minority)...")

# hitung jumlah gambar per kelas
class_sizes = {}
for label in USE_CLASSES:
    folder = os.path.join(DATASET_PATH, label)
    class_sizes[label] = len(glob.glob(folder + "/*.jpg"))

# kelas minor otomatis (threshold < 1450)
minor_classes = [label for label, n in class_sizes.items() if n < 1450]
print("\nMinor classes:", minor_classes)

# mulai ekstraksi + augment
for label in USE_CLASSES:

    folder = os.path.join(DATASET_PATH, label)
    img_files = glob.glob(folder + "/*.jpg")

    print(f"Processing {label} – {len(img_files)} images")

    is_minor = label in minor_classes

    for img_path in img_files:


        feat = extract_features_from_path(img_path)
        X.append(feat)
        y_list.append(label)
        paths.append(img_path)

        if is_minor:
            img = advanced_preprocess(img_path)
            for aug in augment_image(img):
                feat_aug = extract_features_from_image(aug)
                X.append(feat_aug)
                y_list.append(label)
                paths.append(img_path + "_aug")

"""**Step 5: Training XGBoost, Handling Class Imbalance**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
X = np.array(X)
y = le.fit_transform(y_list)

X_train, X_test, y_train, y_test, paths_train, paths_test = train_test_split(
    X, y, paths,
    test_size=0.2,
    random_state=42,
    stratify=y
)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)
from imblearn.over_sampling import SMOTE

sm = SMOTE(k_neighbors=3, random_state=42)
X_train_smote, y_train_smote = sm.fit_resample(X_train_scaled, y_train)

import xgboost as xgb
import numpy as np
from sklearn.utils.class_weight import compute_sample_weight
import os
import joblib
from xgboost.callback import TrainingCallback

# ===============================================
# 1. HITUNG CLASS WEIGHTS
# ===============================================
sample_weights = compute_sample_weight(
    class_weight='balanced',
    y=y_train_smote
)

# ===============================================
# 2. BUAT DMATRIX
# ===============================================
dtrain = xgb.DMatrix(X_train_smote, label=y_train_smote, weight=sample_weights)
dtest  = xgb.DMatrix(X_test_scaled,  label=y_test)

# ===============================================
# 3. PARAMETER XGBOOST
# ===============================================
params = {
    "objective": "multi:softprob",
    "num_class": len(le.classes_),

    # Booster
    "tree_method": "hist",
    "grow_policy": "lossguide",

    # Structure
    "max_depth": 7,
    "max_leaves": 64,
    "min_child_weight": 1,
    "gamma": 0.1,

    # Sampling
    "subsample": 0.85,
    "colsample_bytree": 0.8,
    "colsample_bylevel": 0.8,

    # Learning rate
    "eta": 0.03,

    # Regularization
    "reg_lambda": 2.0,
    "reg_alpha": 0.5,

    # Multi-class stabilization
    "max_delta_step": 1,

    "eval_metric": "mlogloss",
    "seed": 42
}

# ===============================================
# 4. CALLBACK UNTUK SAVE BEST MODEL
# ===============================================
class SaveBestModel(TrainingCallback):
    def __init__(self, save_path):
        self.save_path = save_path
        self.best_score = float("inf")

    def after_iteration(self, model, epoch, evals_log):
        current_score = evals_log['test']['mlogloss'][-1]
        if current_score < self.best_score:
            self.best_score = current_score
            model.save_model(self.save_path)
            print(f"[BEST MODEL] iter={epoch}, loss={current_score:.4f} → saved!")
        return False

# ===============================================
# 5. CALLBACK SAVE CHECKPOINT PER 100 ITERASI
# ===============================================
# class SaveEveryN(TrainingCallback):
#     def __init__(self, save_dir, n=100):
#         self.save_dir = save_dir
#         self.n = n
#         os.makedirs(save_dir, exist_ok=True)

#     def after_iteration(self, model, epoch, evals_log):
#         if epoch % self.n == 0:
#             filepath = f"{self.save_dir}/model_{epoch}.json"
#             model.save_model(filepath)
#             print(f"[CHECKPOINT] saved at iter {epoch}")
#         return False

# ===============================================
# 6. TRAINING + AUTOSAVE
# ===============================================
best_model_path = "/content/drive/MyDrive/xgb_best.json"
checkpoint_dir  = "/content/drive/MyDrive/XGB_Checkpoints"

# model = xgb.train(
#     params,
#     dtrain,
#     num_boost_round=1500,
#     evals=[(dtrain, "train"), (dtest, "test")],
#     early_stopping_rounds=80,
#     verbose_eval=100,
#     callbacks=[
#         SaveBestModel(best_model_path),
#         SaveEveryN(checkpoint_dir, n=100)
#     ]
# )

class SaveEveryN(TrainingCallback):
    def __init__(self, save_dir, start_iter, n=100):
        self.save_dir = save_dir
        self.start_iter = start_iter
        self.n = n
        os.makedirs(save_dir, exist_ok=True)

    def after_iteration(self, model, epoch, evals_log):
        real_iter = self.start_iter + epoch

        if real_iter % self.n == 0:
            filepath = f"{self.save_dir}/model_{real_iter}.json"
            model.save_model(filepath)
            print(f"[CHECKPOINT] saved at iter {real_iter}")

        return False


# kalo kamu ke disconnect bisa gunain ini
resume_model_path = "/content/drive/MyDrive/XGB_Checkpoints/model_1000.json"

model = xgb.train(
    params,
    dtrain,
    num_boost_round=100,  # SISA ITERASI
    evals=[(dtrain, "train"), (dtest, "test")],
    early_stopping_rounds=80,
    verbose_eval=100,
    xgb_model=resume_model_path,  # INI KUNCINYA
    callbacks=[
        SaveBestModel(best_model_path),
        SaveEveryN(checkpoint_dir, start_iter=1000, n=100)
    ]
)


print("\n===== TRAINING SELESAI =====")
print("Best model saved to:", best_model_path)

# ===============================================
# 7. SAVE JOBLIB: MODEL, LABEL ENCODER, SCALER
# ===============================================
booster = model  # model dari xgb.train = Booster object

joblib.dump(booster, "/content/drive/MyDrive/xgboost_model.pkl")
joblib.dump(le, "/content/drive/MyDrive/label_encoder.pkl")
joblib.dump(scaler, "/content/drive/MyDrive/scaler.pkl")

print("Joblib files saved to Google Drive.")

!ls -lh /content

import joblib
joblib.dump(model, "xgboost_model.pkl")
joblib.dump(le, "label_encoder.pkl")
joblib.dump(scaler, "scaler.pkl")

"""# **Step 6: Evaluation(Accuracy, Classification Report, Confusion Matrix, Heatmap, Macro, Micro, Weighted F1 Score, ROC)**"""

from sklearn.metrics import *

pred_prob = model.predict(dtest)      # probabilitas
y_pred = np.argmax(pred_prob, axis=1) # kelas final

print("Accuracy:", accuracy_score(y_test, y_pred) * 100, "%")
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""# **Step 7: F1-Score, ROC-Curve, AUC**"""

from sklearn.metrics import f1_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import numpy as np


print("F1 Macro:", f1_score(y_test, y_pred, average="macro"))
print("F1 Micro:", f1_score(y_test, y_pred, average="micro"))
print("F1 Weighted:", f1_score(y_test, y_pred, average="weighted"))

classes = le.classes_
n_classes = len(classes)


probs = model.predict(dtest)


y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))

plt.figure(figsize=(10, 8))


for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], probs[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{classes[i]} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], "k--")
plt.title("ROC Curve (One-vs-Rest)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

"""**T-SNE Visualization**"""

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

X_scaled = scaler.transform(X)

pca_tsne = PCA(n_components=50, random_state=42)
X_pca = pca_tsne.fit_transform(X_scaled)

X_tsne = TSNE(
    n_components=2,
    learning_rate='auto',
    perplexity=30,
    init='pca',
    random_state=42
).fit_transform(X_pca)

plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=10)
plt.legend(*scatter.legend_elements(), title="Classes")
plt.title("t-SNE Visualization of Feature Space")
plt.show()

"""**Missclasified Samples**"""

from skimage.io import imread
import matplotlib.pyplot as plt
import numpy as np
import os

dtest = xgb.DMatrix(X_test_scaled)
probs = model.predict(dtest)
y_pred = np.argmax(probs, axis=1)

mis_idx = np.where(y_pred != y_test)[0]

print("Total misclassified samples:", len(mis_idx))

num_show = 12
plt.figure(figsize=(15, 12))

shown = 0

for idx in mis_idx:
    if shown >= num_show:
        break

    true_label = le.inverse_transform([y_test[idx]])[0]
    pred_label = le.inverse_transform([y_pred[idx]])[0]
    img_path = paths_test[idx]

    # ---------------------------------------------
    # FIX 1: Kalau file tidak ada, coba remove "_aug"
    # ---------------------------------------------
    if not os.path.exists(img_path):
        if img_path.endswith("_aug"):
            fixed_path = img_path.replace("_aug", "")
            if os.path.exists(fixed_path):
                img_path = fixed_path
            else:
                print("Skip (file tidak ada):", img_path)
                continue
        else:
            print("Skip (file tidak ada):", img_path)
            continue

    # ---------------------------------------------
    # FIX 2: Load aman
    # ---------------------------------------------
    try:
        img = imread(img_path)
    except Exception as e:
        print("Gagal load:", img_path, "error:", e)
        continue

    plt.subplot(3, 4, shown + 1)
    plt.imshow(img)
    plt.axis('off')
    plt.title(f"True: {true_label}\nPred: {pred_label}")

    shown += 1

plt.tight_layout()
plt.show()

"""# **Error Distribution**"""

from collections import Counter
err_true = le.inverse_transform(y_test[mis_idx])
print("Misclassified counts by TRUE class:")
print(Counter(err_true))
err_pred = le.inverse_transform(y_pred[mis_idx])
print("\nMisclassified counts by PREDICTED class:")
print(Counter(err_pred))